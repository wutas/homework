{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import team_utils\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dspl.db import TeraDataDB\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "from dspl.models import classifiers\n",
    "from scoring_pipeline.pipeline import JupyterNotebookPipeline\n",
    "from scoring_pipeline.transformer import Estimator\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "import teradatasql\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('score/for_load.pkl', 'rb') as handle:\n",
    "    for_load = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гузим модели и энкедоры: ZIP(номер модели,кол-во фичей в ней)\n",
    "\n",
    "models_path = dict()\n",
    "\n",
    "for i, n in zip([0, 1, 2, 8], [8, 8, 6, 8]):\n",
    "    models_path[i] = f'saveruns/less_features/runs/{i}/models/CatBoost.{n}.Perm_{i}.pkl'\n",
    "\n",
    "for i, n in zip([3, 4, 5, 6, 7], [5, 18, 11, 11, 7]):\n",
    "    models_path[i] = f'saveruns/less_features/runs/{i}/models/calib_CatBoost.{n}.Perm_{i}/isotonic.pkl'\n",
    "\n",
    "encoders_path = dict()\n",
    "\n",
    "for i, n in zip([0, 1, 2, 8], [8, 8, 6, 8]):\n",
    "    encoders_path[i] = f'saveruns/less_features/runs/{i}/models/encoder.pkl'\n",
    "\n",
    "for i, n in zip([3, 4, 5, 6, 7], [5, 18, 11, 11, 7]):\n",
    "    encoders_path[i] = f'saveruns/less_features/runs/{i}/models/calib_CatBoost.{n}.Perm_{i}/encoder.pkl'\n",
    "\n",
    "mod_enc = dict()\n",
    "for i, j in zip(models_path, encoders_path):\n",
    "    with open(models_path[i], \"rb\") as cds:\n",
    "        model = pickle.load(cds)\n",
    "    with open(encoders_path[j], \"rb\") as cds:\n",
    "        encoder = pickle.load(cds)\n",
    "    mod_enc[i] = [model, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_proba(df, mod_enc):\n",
    "    '''\n",
    "    Функция для параллельного скоринга моделей\n",
    "\n",
    "\n",
    "    Parameters\n",
    "        ----------\n",
    "        df: pandas.df\n",
    "            Фичи для модели, не итерируются\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mod_enc: model, lenght = 9\n",
    "            На вход подается список моделей, по которым происходит параллельная итерация\n",
    "    '''\n",
    "    proc_id = multiprocessing.current_process()._identity[0]\n",
    "    model = mod_enc[0]\n",
    "    encoder = mod_enc[1]\n",
    "    print('Read model_encoder', proc_id)\n",
    "    df = encoder.transform(df)\n",
    "    sc = model.transform(df)\n",
    "\n",
    "    score = pd.DataFrame()\n",
    "    score['epk_id'] = df.epk_id\n",
    "    score[int(encoder.get_params()['config']['oot_data_path'][5])] = sc\n",
    "    print('Score DONE', proc_id)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('scoring_data/Data/batch'):\n",
    "    os.makedirs('scoring_data/Data/batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Батчами по 700к наблюдейний грузим фичи, и параллельно скорим их 9ю моделями\n",
    "for i, df in enumerate(pd.read_csv(for_load['path'],\n",
    "                                   chunksize=700000,\n",
    "                                   sep=';', header=None,\n",
    "                                   names=df_crm.columns,\n",
    "                                   dtype={'epk_id': 'int64'})):\n",
    "    print(f'-------------------{i}------------------')\n",
    "    if __name__ == '__main__':\n",
    "        if i == 0:\n",
    "            pool = multiprocessing.Pool(processes=9)\n",
    "        df.to_csv(f'test.csv', index=False)\n",
    "\n",
    "        f_score = partial(score_proba, df)\n",
    "        score = pool.map(f_score, mod_enc.values())\n",
    "\n",
    "        for j, df_score in enumerate(score):\n",
    "            if j == 0:\n",
    "                score_all_models = df_score\n",
    "            else:\n",
    "                score_all_models[df_score.columns[1]] = df_score.iloc[:, 1]\n",
    "        del score\n",
    "        score_all_models.to_csv(f'scoring_data/Data/batch/{i}.csv', index=False)\n",
    "        del df  \n",
    "        del score_all_models\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
