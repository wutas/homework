{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import team_utils\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dspl.db import TeraDataDB\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "from dspl.models import classifiers\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "import teradatasql\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from teradata_tools import td_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = 20  # Кол-во процессов и одновременно заливаемы батчей\n",
    "\n",
    "\n",
    "def get_proc_id(i):  # Получаем номер процесса\n",
    "    print(multiprocessing.current_process())\n",
    "    return multiprocessing.current_process()._identity[0]\n",
    "\n",
    "\n",
    "def create_table(table_name, proc_id):  # Создаем собственную таблицу под кажды процесс\n",
    "\n",
    "    table_name_id = table_name + str(proc_id)\n",
    "\n",
    "    tdHost = ''\n",
    "    tdUser = ''\n",
    "    tdPwd = ''\n",
    "\n",
    "    connection = teradatasql.connect(\n",
    "        host=tdHost,\n",
    "        user=tdUser,\n",
    "        password=tdPwd\n",
    "    )\n",
    "\n",
    "    cursor_create = connection.cursor()\n",
    "\n",
    "    cursor_create.execute(f'''\n",
    "    CREATE MULTISET TABLE {table_name_id} ,NO FALLBACK ,\n",
    "         NO BEFORE JOURNAL,\n",
    "         NO AFTER JOURNAL,\n",
    "         CHECKSUM = DEFAULT,\n",
    "         DEFAULT MERGEBLOCKRATIO,\n",
    "         MAP = TD_MAP2\n",
    "         (\n",
    "            EPK_ID BIGINT,\n",
    "            \"0\" DECIMAL(5,4),\n",
    "            \"1\" DECIMAL(5,4),\n",
    "            \"2\" DECIMAL(5,4),\n",
    "            \"3\" DECIMAL(5,4),\n",
    "            \"4\" DECIMAL(5,4),\n",
    "            \"5\" DECIMAL(5,4),\n",
    "            \"6\" DECIMAL(5,4),\n",
    "            \"7\" DECIMAL(5,4),\n",
    "            \"8\" DECIMAL(5,4)\n",
    ")\n",
    "    PRIMARY INDEX ( EPK_ID );\n",
    "    ''')\n",
    "\n",
    "    cursor_create.execute(f'''\n",
    "    grant all on {table_name_id} to \"\";\n",
    "    ''')\n",
    "    \n",
    "    connection.close()\n",
    "    del connection\n",
    "    print('Create: ', table_name_id)\n",
    "    pass \n",
    "\n",
    "\n",
    "def create_conect(i):  # открываем конекшены\n",
    "    if i % 2 == 0:\n",
    "        tdHost = ''\n",
    "        tdUser = ''\n",
    "        tdPwd = ''\n",
    "\n",
    "        connection = teradatasql.connect(\n",
    "            host=tdHost,\n",
    "            user=tdUser,\n",
    "            password=tdPwd\n",
    "        )\n",
    "        print('get_connect: ', tdUser,i)\n",
    "    else:\n",
    "        tdHost = ''\n",
    "        tdUser = ''\n",
    "        tdPwd = '' \n",
    "\n",
    "        connection = teradatasql.connect(\n",
    "            host=tdHost,\n",
    "            user=tdUser,\n",
    "            password=tdPwd\n",
    "        )\n",
    "        print('get_connect: ', tdUser,i)\n",
    "    return connection\n",
    "\n",
    "\n",
    "def close_conect(connection):  # Закрываем конекшены\n",
    "    proc_id = multiprocessing.current_process()._identity[0]\n",
    "    connection.close()\n",
    "    print('Close_con: ', proc_id)\n",
    "    pass\n",
    "\n",
    "\n",
    "def drop_table(table_name,connection):  # Удаляем временные талицы\n",
    "    proc_id = multiprocessing.current_process()._identity[0]\n",
    "    table_name = table_name + str(proc_id)\n",
    "    cursor_drop = connection.cursor()\n",
    "    \n",
    "    cursor_drop.execute(f'''\n",
    "    DROP TABLE {table_name}\n",
    "    ''')\n",
    "    \n",
    "    print('Drop: ', table_name)\n",
    "    pass \n",
    "\n",
    "\n",
    "def merge(proc_id,table_name):  # Соединяем временные таблицы\n",
    "    union = str()\n",
    "    for n in proc_id:\n",
    "        table_name_id = table_name + str(n)\n",
    "        union += f'(SELECT * FROM {table_name_id}) UNION ALL '\n",
    "\n",
    "    tdHost = ''\n",
    "    tdUser = ''\n",
    "    tdPwd = '' \n",
    "\n",
    "    connection_merge = teradatasql.connect(\n",
    "        host=tdHost,\n",
    "        user=tdUser,\n",
    "        password=tdPwd\n",
    "    )\n",
    "\n",
    "    cursor_merge = connection_merge.cursor()\n",
    "    \n",
    "    cursor_merge.execute(f'''\n",
    "    DELETE FROM {table_name[:-1]} \n",
    "    ''')\n",
    "\n",
    "    cursor_merge.execute(f'''\n",
    "    INSERT INTO {table_name[:-1]} (epk_id, \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\")\n",
    "    SELECT agr.epk_id, agr.\"0\", agr.\"1\", agr.\"2\", agr.\"3\", agr.\"4\", agr.\"5\", agr.\"6\", agr.\"7\", agr.\"8\" \n",
    "    FROM ({union[:-10]}) as agr\n",
    "    ''')\n",
    "    connection_merge.close()\n",
    "    \n",
    "    print(f'Merage to {table_name[:-1]} DONE')\n",
    "    pass\n",
    "\n",
    "\n",
    "def csv_to_td_amazing(connection, path, table_name, batches): \n",
    "    '''\n",
    "    Функция для параллельноной перекладки батчей в teradata\n",
    "\n",
    "\n",
    "    Parameters\n",
    "        ----------\n",
    "        connection: teradata connection (list)\n",
    "            Список открытых соединений\n",
    "\n",
    "        path: str\n",
    "            Путь до папки с батчами\n",
    "\n",
    "        table_name: str\n",
    "            Название временных таблиц\n",
    "\n",
    "        batches: str (list)\n",
    "            Список из назания батчей, по которым происходит параллельная выгрузка\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mod_enc: model, lenght = 9\n",
    "            На ваход подается список моделей, по которым происходит параллельная итерация\n",
    "    '''\n",
    "    \n",
    "    proc_id = multiprocessing.current_process()._identity[0]\n",
    "    df = pd.read_csv(path + batches, dtype={'epk_id': 'int64'})\n",
    "    df = df[df['epk_id'] > 0]\n",
    "    print(f'{batches} shape: {df.shape} {proc_id}')\n",
    "    tup = [tuple([int(x[0]), float(x[1]), float(x[2]), float(x[3]), float(x[4]), float(x[5]),\n",
    "                  float(x[6]), float(x[7]), float(x[8]), float(x[9])])\n",
    "           for x in df.to_records(index=False)]\n",
    "    conn = connection[(proc_id - 1) % len(connection)]\n",
    "    cursor = conn.cursor()\n",
    "    table_name = table_name + str(proc_id)\n",
    "    stmt = f'''INSERT INTO {table_name} (epk_id, \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\")\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)'''\n",
    "    print(f'INSERTING: {batches} to {table_name}, {proc_id}')\n",
    "    cursor.executemany(stmt, tup)\n",
    "    print('insert_done: ',proc_id)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(processes=batch_count)\n",
    "    proc_id = pool.map(get_proc_id, range(batch_count))\n",
    "\n",
    "    if len(set(proc_id)) == batch_count:\n",
    "\n",
    "        path = 'scoring_data/Data/batch/'\n",
    "        table_name = f''\n",
    "\n",
    "        f_careate = partial(create_table, table_name)\n",
    "        pool.map(f_careate, proc_id)\n",
    "\n",
    "        connection = pool.map(create_conect, range(batch_count))\n",
    "\n",
    "        f_insert = partial(csv_to_td_amazing, connection, path, table_name)\n",
    "        batches = os.listdir(path)\n",
    "        batches = [bat for bat in batches if bat[0] != \".\"]\n",
    "        %time pool.map(f_insert, batches)\n",
    "\n",
    "        merge(proc_id, table_name)\n",
    "\n",
    "        f_drop = partial(drop_table, table_name)\n",
    "        pool.map(f_drop, connection)\n",
    "\n",
    "        pool.map(close_conect, connection)\n",
    "        del connection\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    del pool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
